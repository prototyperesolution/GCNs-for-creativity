{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c13b5676",
   "metadata": {},
   "source": [
    "Experimental condition 4: Appraiser's reconstruction is used to train appraiser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "45b7eb98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow import Tensor\n",
    "import tensorflow_probability as tfp\n",
    "import cv2\n",
    "import glob\n",
    "import gym\n",
    "import random\n",
    "from gym import Env\n",
    "from gym.spaces import Discrete, Box\n",
    "import math\n",
    "import PIL\n",
    "import PIL.ImageDraw\n",
    "import PIL.Image\n",
    "from PIL import ImageFont\n",
    "import os\n",
    "\"\"\"making tf allocate memory dynamically, to make the running of the program more efficiently\"\"\"\n",
    "tf.compat.v1.enable_eager_execution()\n",
    "config = tf.compat.v1.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "sess = tf.compat.v1.Session(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9cad7f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"creates relevant directory where model weights will be saved\"\"\"\n",
    "os.mkdir('experimental_condition_four')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d10c7e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"selects 1000 images randomly from a directory of saved images, here images/images/\"\"\"\n",
    "\"\"\"these images are used to train the autoencoder\"\"\"\n",
    "\n",
    "import os\n",
    "filenames = [img for img in glob.glob(\"images/images/*/*.jpg\")]\n",
    "np.random.shuffle(filenames)\n",
    "filenames = filenames[:1000]\n",
    "images = [cv2.imread(img) for img in filenames]\n",
    "for i in range(0,len(images)):\n",
    "    images[i] = cv2.cvtColor(images[i], cv2.COLOR_BGR2RGB)\n",
    "    images[i] = cv2.resize(images[i], (400,400))\n",
    "images = np.array(images)\n",
    "images = images.astype('float32')/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6aee77cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"custom layer which produces the distribution which is sampled in the VAE\"\"\"\n",
    "\n",
    "class Latent_features(tf.keras.layers.Layer):\n",
    "    def call(self, inputs) -> Tensor:\n",
    "        dist_mean, dist_log_var = inputs\n",
    "        batch = tf.shape(dist_mean)[0]\n",
    "        dim = tf.shape(dist_mean)[1]\n",
    "        epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n",
    "        return dist_mean + tf.exp(0.5 * dist_log_var) * epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d5e66a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(tf.keras.Model):\n",
    "    \n",
    "    \"\"\"builds encoder and decoder sub-networks\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(VAE, self).__init__()\n",
    "        self.conv_layer_steps = [2,2,2,2]\n",
    "        self.encoder = self.build_encoder()\n",
    "        self.decoder = self.build_decoder()\n",
    "        self.total_loss_tracker = keras.metrics.Mean(name=\"total_loss\")\n",
    "        self.reconstruction_loss_tracker = keras.metrics.Mean(name=\"reconstruction_loss\")\n",
    "        self.kl_loss_tracker = keras.metrics.Mean(name=\"kl_loss\")\n",
    "        \n",
    "        autoencoder_input = keras.Input(shape=(400,400,3))\n",
    "        encoded_img = self.encoder(autoencoder_input)[2]\n",
    "        decoded_img = self.decoder(encoded_img)\n",
    "        self.autoencoder = keras.Model(autoencoder_input, decoded_img)\n",
    "    \n",
    "    \"\"\"custom call function necessary for custom keras model\"\"\"\n",
    "    def call(self, image):\n",
    "        encoded = self.encoder(image)[2]\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded\n",
    "    \n",
    "    \"\"\"utility function for batch norm and leakyrelu\"\"\"\n",
    "    def activation_block(self, inputs) -> Tensor:\n",
    "        BN = tf.keras.layers.BatchNormalization(momentum=0.99) (inputs)\n",
    "        activated = tf.keras.layers.LeakyReLU()(BN)\n",
    "        return activated\n",
    "    \n",
    "    \"\"\"implements skip connection for the resnet component of the architecture\"\"\"\n",
    "    def residual_block(self, inputs: Tensor, scale_change: bool, polarity, kernel_size = 2, filters = 64, stride_size = 2) -> Tensor:\n",
    "        #POLARITY IS 0 IF DOWNSCALING (ENCODER) 1 IF UPSCALING (DECODER)\n",
    "        if polarity == 0:\n",
    "            output_1 = tf.keras.layers.Conv2D(filters, (kernel_size, kernel_size), strides = (1 if not scale_change else stride_size), padding = 'same') (inputs)\n",
    "            output_1 = self.activation_block(output_1)\n",
    "            output_1 = tf.keras.layers.Conv2D(filters, (kernel_size, kernel_size), strides = (1), padding = 'same') (output_1)\n",
    "            if scale_change == True:\n",
    "                inputs = tf.keras.layers.Conv2D(filters, (kernel_size, kernel_size), strides = (stride_size), padding = 'same') (inputs)\n",
    "        \n",
    "        else:\n",
    "            output_1 = tf.keras.layers.Conv2DTranspose(filters, (kernel_size, kernel_size), strides = (1 if not scale_change else stride_size), padding = 'same') (inputs)\n",
    "            output_1 = self.activation_block(output_1)\n",
    "            output_1 = tf.keras.layers.Conv2DTranspose(filters, (kernel_size, kernel_size), strides = (1), padding = 'same') (output_1)\n",
    "            if scale_change == True:\n",
    "                inputs = tf.keras.layers.Conv2DTranspose(filters, (kernel_size, kernel_size), strides = (stride_size), padding = 'same') (inputs)\n",
    "        \n",
    "        output_2 = tf.keras.layers.Add()([inputs, output_1])\n",
    "        output_2 = self.activation_block(output_2)\n",
    "        \n",
    "        return(output_2)\n",
    "        \n",
    "    \"\"\"returns the encoder model, a sub-architecture of the autoencoder\"\"\"\n",
    "    def build_encoder(self):\n",
    "        num_filters = 32\n",
    "        conv_layer_steps = self.conv_layer_steps\n",
    "        E_input = tf.keras.layers.Input(shape = (400,400,3), name = 'original_image')\n",
    "        E = tf.keras.layers.Conv2D(num_filters, (1,1), strides=(1), padding = 'same') (E_input)\n",
    "        E = self.activation_block(E)\n",
    "        \n",
    "        block_depths = [2,5,5,2]\n",
    "        filters = [32,64,64,128]\n",
    "        \n",
    "        for i in range(0,len(block_depths)):\n",
    "            for j in range(0,block_depths[i]):\n",
    "                E = self.residual_block(E, (j==0 and i!=0), 0, 2, filters[i], 2)\n",
    "\n",
    "        \n",
    "            \n",
    "        self.reshape_dims = E.shape\n",
    "            \n",
    "        E = tf.keras.layers.Flatten() (E)\n",
    "        \n",
    "        self.flatten_dims = E.shape\n",
    "        \n",
    "        E = (tf.keras.layers.LeakyReLU()) (E)\n",
    "        \n",
    "        distribution_mean = tf.keras.layers.Dense(16, name='mean')(E)\n",
    "        distribution_variance = tf.keras.layers.Dense(16, name='log_variance')(E)\n",
    "        latent_encoding = Latent_features()([distribution_mean, distribution_variance])\n",
    "        \n",
    "        \n",
    "        encoder = keras.Model(E_input, [distribution_mean, distribution_variance, latent_encoding], name=\"encoder\")\n",
    "        \n",
    "                \n",
    "        return encoder\n",
    "    \n",
    "    \"\"\"returns the decoder model, a sub-network for autoencoder architecture\"\"\"\n",
    "    def build_decoder(self):\n",
    "        num_filters = 256\n",
    "        conv_layer_steps = self.conv_layer_steps\n",
    "        decoder_input = keras.Input(shape=(16,))\n",
    "        D = tf.keras.layers.LeakyReLU() (decoder_input)\n",
    "        D = (tf.keras.layers.Dense(self.flatten_dims[1])) (D)\n",
    "        D = (tf.keras.layers.LeakyReLU()) (D)\n",
    "        D = (tf.keras.layers.Reshape((self.reshape_dims[1], self.reshape_dims[2], self.reshape_dims[3]))) (D)\n",
    "        \n",
    "        block_depths = [2,5,5,2]\n",
    "        filters = [128,64,64,32]\n",
    "        \n",
    "        for i in range(0,len(block_depths)):\n",
    "            for j in range(0,block_depths[i]):\n",
    "                D = self.residual_block(D, (j==0 and i!=0), 1, 2, filters[i], 2)\n",
    "            \n",
    "        D = tf.keras.layers.Conv2DTranspose(3, (1,1), strides = (1), padding = 'same') (D)\n",
    "        D = self.activation_block(D)\n",
    "        \n",
    "        \n",
    "        decoded = (tf.keras.layers.Activation('sigmoid')) (D)\n",
    "        \n",
    "        decoder = tf.keras.Model(inputs = decoder_input, outputs = decoded)\n",
    "        \n",
    "        return decoder\n",
    "\n",
    "    \n",
    "    \"\"\"necessary for loss trackers used for custom loss function\"\"\"\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [self.total_loss_tracker,\n",
    "            self.reconstruction_loss_tracker,\n",
    "            self.kl_loss_tracker]\n",
    "\n",
    "    def train_step(self, data):\n",
    "        with tf.GradientTape() as tape:\n",
    "            mean, log_var, latent = self.encoder(data)\n",
    "            reconstruction = self.decoder(latent)\n",
    "            reconstruction_loss = tf.reduce_mean(tf.reduce_sum(keras.losses.binary_crossentropy(data, reconstruction), axis=(1, 2)))\n",
    "            kl_loss = -0.5 * (1 + log_var - tf.square(mean) - tf.exp(log_var))\n",
    "            kl_loss = tf.reduce_mean(tf.reduce_sum(kl_loss, axis=1))\n",
    "            total_loss = reconstruction_loss + kl_loss\n",
    "        grads = tape.gradient(total_loss, self.trainable_weights)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
    "        self.total_loss_tracker.update_state(total_loss)\n",
    "        self.reconstruction_loss_tracker.update_state(reconstruction_loss)\n",
    "        self.kl_loss_tracker.update_state(kl_loss)\n",
    "        return {\"loss\": self.total_loss_tracker.result(),\n",
    "            \"reconstruction_loss\": self.reconstruction_loss_tracker.result(),\n",
    "            \"kl_loss\": self.kl_loss_tracker.result(),}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "396836f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <bound method Latent_features.call of <__main__.Latent_features object at 0x000001FB0D4A8808>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Constant'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <bound method Latent_features.call of <__main__.Latent_features object at 0x000001FB0D4A8808>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Constant'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    }
   ],
   "source": [
    "\"\"\"compiling the VAE with RMSprop, and a lower learning rate than during the VAE training loop\"\"\"\n",
    "opt = tf.keras.optimizers.RMSprop(learning_rate = 0.0000005)\n",
    "vae = VAE()\n",
    "\n",
    "vae.compile(optimizer=opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5476a85d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"training VAE\"\"\"\n",
    "vae.fit(images, epochs = 20, batch_size = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "741d2681",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Painting_Env(gym.Env):\n",
    "  def __init__(self, paintings_per_loop = 100, maximum_acceptable_loss = 0.005):\n",
    "\n",
    "    super().__init__()\n",
    "    self.action_space = Box(low = 0, high = 1, shape = (64,64,3))\n",
    "    self.observation_space = Box(low = 0, high = 1, shape = (64,64,3))\n",
    "    self.state = np.zeros((64,64,3))\n",
    "    self.paintings_per_loop = paintings_per_loop\n",
    "    self.paintings_per_loop_original = self.paintings_per_loop\n",
    "    \n",
    "    self.maximum_acceptable_loss = maximum_acceptable_loss\n",
    "\n",
    "\n",
    "\n",
    "  def step(self, action):\n",
    "    \"\"\"resizing action to original_img to use on vae\"\"\"\n",
    "    self.state = action.squeeze()\n",
    "    original_img = cv2.resize(self.state, (400,400))\n",
    "\n",
    "    current_weights = vae.get_weights()\n",
    "    \"\"\"creating reconstructed img\"\"\"\n",
    "    reconstructed_orig = vae(original_img.reshape(1,400,400,3)).numpy().squeeze()\n",
    "    \"\"\"training vae on reconstructed img\"\"\"\n",
    "    vae.train_step(reconstructed_orig.reshape(1,400,400,3))\n",
    "\n",
    "    new_weights = vae.get_weights()\n",
    "\n",
    "    weight_diff = []\n",
    "\n",
    "    reconstructed = vae(reconstructed_orig.reshape(1,400,400,3)).numpy().squeeze()\n",
    "    \"\"\"finding weighted avg of weight differences\"\"\"\n",
    "    for i in range(1,len(current_weights)-1):\n",
    "      if current_weights[i].ndim != 0:\n",
    "        weight_diff.append((tf.math.reduce_mean(np.abs(tf.keras.losses.MSE(current_weights[i],new_weights[i]).numpy())).numpy())*(len(current_weights)/2 - (abs(i-len(current_weights)/2))))\n",
    "    \"\"\"making this a single number\"\"\"\n",
    "    weight_diff = tf.math.reduce_mean(weight_diff).numpy()\n",
    "\n",
    "    loss_val = tf.math.reduce_mean(tf.keras.losses.MSE(original_img, reconstructed)).numpy()\n",
    "    \"\"\"if loss is above maximum acceptable loss, fix weight difference and revert changes to vae\"\"\"\n",
    "    if loss_val > self.maximum_acceptable_loss:\n",
    "      weight_diff = 0.0001\n",
    "      vae.set_weights(current_weights)\n",
    "    else:\n",
    "      weight_diff *= 1e8\n",
    "\n",
    "    self.reconstructed = reconstructed\n",
    "\n",
    "    loss = (original_img - reconstructed)\n",
    "    loss = np.abs(loss)\n",
    "    \"\"\"ensuring no division by 0\"\"\"\n",
    "    loss[loss == 0] = 0.00000001\n",
    "    if np.array_equal(np.full((400,400,3),0.00000001), loss):\n",
    "      print('loss 0')\n",
    "    \n",
    "\n",
    "    \"\"\"resizing reward to 64,64,3\"\"\"    \n",
    "    reward = -(loss/weight_diff)\n",
    "    reward = cv2.resize(reward, (64,64))\n",
    "\n",
    "    self.paintings_per_loop -= 1\n",
    "    self.state = cv2.resize(self.reconstructed, (64,64))\n",
    "    self.reward = reward\n",
    "\n",
    "    if self.paintings_per_loop <= 0:\n",
    "      done = True\n",
    "    else:\n",
    "      done = False\n",
    "    self.loss = loss\n",
    "    info = {}\n",
    "    return self.state, reward, done, info\n",
    "\n",
    "\n",
    "  def render(self, mode = None):\n",
    "    \"\"\"utility function to render environment at any time step\"\"\"\n",
    "    original_img = (cv2.resize(self.state, (400,400)))\n",
    "    img = PIL.Image.fromarray(((original_img)*255).astype('uint8'))\n",
    "    reconstructed = ((self.reconstructed*255).astype('uint8')).squeeze()\n",
    "    if mode == 'human':\n",
    "      d = PIL.ImageDraw.Draw(img)\n",
    "      font = ImageFont.load_default()\n",
    "      y = 0\n",
    "      _, height = d.textsize(\"W\", font)\n",
    "      d.text((0, y), f\"Reward: {tf.math.reduce_mean(self.reward)}\", fill=(0, 255, 0))\n",
    "      y+=height\n",
    "      d.text((0, y), f\"Loss: {tf.math.reduce_mean(self.loss)}\", fill=(0, 255, 0))\n",
    "      y+=height\n",
    "      reconstructed = ((self.reconstructed*255).astype('uint8')).squeeze()\n",
    "      img = np.array(img)\n",
    "      img = np.concatenate((img, reconstructed), axis = 1)\n",
    "    else:\n",
    "      img = np.array(img)\n",
    "      img = np.concatenate((img, reconstructed), axis = 1)      \n",
    "    return (np.array(img))\n",
    "\n",
    "\n",
    "  def reset(self):\n",
    "    #self.state = np.zeros((100,100,3))\n",
    "    self.paintings_per_loop = self.paintings_per_loop_original\n",
    "    return self.state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5161282b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"registering the gym environment\"\"\"\n",
    "gym.envs.registration.register(\n",
    "    id='painting_environment-v1',\n",
    "    entry_point=f'{__name__}:Painting_Env',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2f88d064",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ppo_memory:\n",
    "  def __init__(self, batch_size):\n",
    "    self.states = []\n",
    "    self.probabilities = [] \n",
    "    self.terminal_memory = []\n",
    "    self.values = []\n",
    "    self.actions = []\n",
    "    self.rewards = []\n",
    "    self.batch_size = batch_size\n",
    "\n",
    "  \"\"\"storing memories\"\"\"\n",
    "  def store_memory(self, state, action, probabilities, values, reward, done):\n",
    "    self.states.append(state)\n",
    "    self.actions.append(action)\n",
    "    self.probabilities.append(probabilities)\n",
    "    self.values.append(values)\n",
    "    self.rewards.append(reward)\n",
    "    self.terminal_memory.append(done)\n",
    "\n",
    "  \"\"\"clearing all memory, happens after a completed learning cycle\"\"\"\n",
    "  def clear_memory(self):\n",
    "    self.states = []\n",
    "    self.probabilities=[]\n",
    "    self.rewards = []\n",
    "    self.terminal_memory = []\n",
    "    self.values = []\n",
    "    self.actions = []\n",
    "\n",
    "  \"\"\"returning states, actions, values (for the critic), probabilities, and the indices of these items\"\"\"\n",
    "  def generate_batches(self):\n",
    "    number_states = len(self.states)\n",
    "    batch_start_points = np.arange(0, number_states, self.batch_size)\n",
    "\n",
    "    indices = np.arange(number_states, dtype=np.int64)\n",
    "\n",
    "    batches = [indices[i:i+self.batch_size] for i in batch_start_points]\n",
    "\n",
    "    return np.array(self.states), np.array(self.actions), np.array(self.probabilities), np.array(self.values), np.array(self.rewards),\\\n",
    "    np.array(self.terminal_memory), batches\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "84f9a5c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"custom keras layer for the actor network, returns a gaussian distribution over every\n",
    "colour channel for every pixel\"\"\"\n",
    "class gaussian_dist(tf.keras.layers.Layer):\n",
    "  def call(self, inputs) -> Tensor:\n",
    "    means, stddevs = inputs\n",
    "    dist = tfp.distributions.Normal(loc = means, scale = stddevs**2)\n",
    "    return dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "89894fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class actor_network(tf.keras.Model):\n",
    "  \n",
    "  def __init__(self, n_actions, conv_layer_params=None, conv_transpose_params=None, lstm_size = None, midpoint_fc_params = None, name = 'actor',\n",
    "               batch_size = 16):\n",
    "    \n",
    "    super(actor_network, self).__init__()\n",
    "\n",
    "    self.n_actions = n_actions\n",
    "    self.model_name = name\n",
    "\n",
    "    self.checkpoint_file = self.model_name+'_ppo.h5'\n",
    "    self.midpoint_fc_params = midpoint_fc_params\n",
    "    self.conv_layer_params = conv_layer_params\n",
    "    flatten_dims = None\n",
    "    \"\"\"finding the flattened dimensions for the bottleneck of the actor (after the LSTM section)\"\"\"\n",
    "    for i in range(0,len(conv_layer_params)):\n",
    "      if not flatten_dims:\n",
    "        flatten_dims = conv_layer_params[i][2]\n",
    "      else:\n",
    "        flatten_dims *= conv_layer_params[i][2]\n",
    "\n",
    "    self.reshape_dims = np.array([n_actions[0],n_actions[1]])/flatten_dims\n",
    "    self.reshape_dims = self.reshape_dims.astype(int)\n",
    "    flatten_dims = (self.reshape_dims[0]*self.reshape_dims[1])*conv_layer_params[-1][0]\n",
    "    self.flatten_dims = flatten_dims\n",
    "\n",
    "    input_encoder = []\n",
    "\n",
    "    \"\"\"building an input encoder, which is convolutional\"\"\"\n",
    "    for config in conv_layer_params:\n",
    "      (filters, kernel_size, strides) = config\n",
    "      input_encoder.append(\n",
    "            tf.keras.layers.Conv2D(\n",
    "                filters=filters,\n",
    "                kernel_size=kernel_size,\n",
    "                strides=strides,\n",
    "                dilation_rate=(1,1),\n",
    "                padding = 'same'))\n",
    "      input_encoder.append(tf.keras.layers.BatchNormalization(momentum=0.99))\n",
    "      input_encoder.append(tf.keras.layers.LeakyReLU())\n",
    "    \n",
    "    \"\"\"building a two layer LSTM network\"\"\"\n",
    "    lstm_network = []\n",
    "    for i in range(2):\n",
    "      lstm_network.append(tf.keras.layers.LSTM(128, return_sequences = True, stateful = True))\n",
    "\n",
    "    lstm_network.append(tf.keras.layers.Dense(flatten_dims))\n",
    "    lstm_network.append(tf.keras.layers.LeakyReLU())\n",
    "    \n",
    "    output_encoder = []\n",
    "    \n",
    "    \"\"\"building an output decoder, which is made of deconvolutional layers\"\"\"\n",
    "    for config in conv_transpose_params:\n",
    "      (filters, kernel_size, strides) = config\n",
    "      output_encoder.append(\n",
    "            tf.keras.layers.Conv2DTranspose(\n",
    "                filters=filters,\n",
    "                kernel_size=kernel_size,\n",
    "                strides=strides,\n",
    "                dilation_rate=(1,1),\n",
    "                padding = 'same'))\n",
    "      output_encoder.append(tf.keras.layers.BatchNormalization(momentum=0.99))\n",
    "      output_encoder.append(tf.keras.layers.LeakyReLU())\n",
    "    \n",
    "    \"\"\"creating the distributions necessary for the custom layer\"\"\"\n",
    "    output_encoder_means = tf.keras.layers.Conv2DTranspose(filters = 3, kernel_size = (1,1), strides = (1,1), dilation_rate=(1,1), padding='same', activation = 'sigmoid')\n",
    "    output_encoder_stddevs = tf.keras.layers.Conv2DTranspose(filters = 3, kernel_size = (1,1), strides = (1,1), dilation_rate=(1,1), padding='same',activation='sigmoid')\n",
    "    output_encoder_dist = gaussian_dist()\n",
    "\n",
    "    self.input_encoder = input_encoder\n",
    "    self.lstm_network = lstm_network\n",
    "    self.output_encoder = output_encoder\n",
    "    self.output_encoder_means = output_encoder_means\n",
    "    self.output_encoder_stddevs = output_encoder_stddevs\n",
    "    self.output_encoder_dist = output_encoder_dist\n",
    "\n",
    "  def call(self, state):\n",
    "\n",
    "      \"\"\"reshaping data to make it appropriate for an LSTM network\"\"\"\n",
    "      if tf.rank(state) == 3:\n",
    "        state = tf.reshape(state, (1,1,self.n_actions[0],self.n_actions[1],self.n_actions[2]))\n",
    "      else:\n",
    "        state = tf.reshape(state, (1,len(state),self.n_actions[0],self.n_actions[1],self.n_actions[2]))\n",
    "      \n",
    "      action_value = self.input_encoder[0](state)\n",
    "      input_shape = np.shape(action_value)\n",
    "      \"\"\"putting data through the input encoder\"\"\"\n",
    "      for i in range(1,len(self.input_encoder)):\n",
    "        action_value = self.input_encoder[i](action_value)\n",
    "      \"\"\"reshaping data to be suitable for lstm network\"\"\"\n",
    "      action_value = tf.reshape(action_value, (1,input_shape[1],self.flatten_dims))\n",
    "      for layer in self.lstm_network:\n",
    "        action_value = layer(action_value)\n",
    "      \"\"\"reshaping data to be suitable for the deconvolutional layers\"\"\"\n",
    "      action_value = tf.reshape(action_value, (input_shape[1],self.reshape_dims[0],self.reshape_dims[1],256))\n",
    "      for layer in self.output_encoder:\n",
    "        action_value = layer(action_value)\n",
    "      \"\"\"generating and returning distribution\"\"\"\n",
    "      action_value_means = self.output_encoder_means(action_value)\n",
    "      action_value_stddevs = self.output_encoder_stddevs(action_value)\n",
    "      dist_channel = self.output_encoder_dist([action_value_means, action_value_stddevs])\n",
    "\n",
    "\n",
    "      return dist_channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "73cdff9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class critic_network(tf.keras.Model):\n",
    "\n",
    "  def __init__(self, n_actions, conv_layer_params=None, conv_transpose_params=None, lstm_size = None, midpoint_fc_params = None, name = 'critic'):\n",
    "    \n",
    "    super(critic_network, self).__init__()\n",
    "\n",
    "    \"\"\"n_actions is the dimensions of the actions available, so in this case 64,64,3\"\"\"\n",
    "    self.n_actions = n_actions\n",
    "    self.model_name = name\n",
    "\n",
    "    self.checkpoint_file = self.model_name+'_ppo.h5'\n",
    "    self.midpoint_fc_params = midpoint_fc_params\n",
    "    self.conv_layer_params = conv_layer_params\n",
    "    flatten_dims = None\n",
    "    \"\"\"finding the flattened dimensions for the bottleneck of the critic (after the LSTM section)\"\"\"\n",
    "    for i in range(0,len(conv_layer_params)):\n",
    "      if not flatten_dims:\n",
    "        flatten_dims = conv_layer_params[i][2]\n",
    "      else:\n",
    "        flatten_dims *= conv_layer_params[i][2]\n",
    "\n",
    "    self.reshape_dims = np.array([n_actions[0],n_actions[1]])/flatten_dims\n",
    "    self.reshape_dims = self.reshape_dims.astype(int)\n",
    "    flatten_dims = (self.reshape_dims[0]*self.reshape_dims[1])*conv_layer_params[-1][0]\n",
    "    self.flatten_dims = flatten_dims\n",
    "\n",
    "    \"\"\"building an input encoder, which is convolutional\"\"\"\n",
    "    input_encoder = []\n",
    "    for config in conv_layer_params:\n",
    "      (filters, kernel_size, strides) = config\n",
    "      input_encoder.append(\n",
    "            tf.keras.layers.Conv2D(\n",
    "                filters=filters,\n",
    "                kernel_size=kernel_size,\n",
    "                strides=strides,\n",
    "                dilation_rate=(1,1),\n",
    "                activation=tf.keras.activations.relu,\n",
    "                padding = 'same'))\n",
    "      input_encoder.append(tf.keras.layers.BatchNormalization(momentum=0.99))\n",
    "      input_encoder.append(tf.keras.layers.LeakyReLU())\n",
    "\n",
    "\n",
    "    lstm_network = []\n",
    "    \"\"\"building a two layer LSTM network\"\"\"\n",
    "    for i in range(2):\n",
    "      lstm_network.append(tf.keras.layers.LSTM(128, return_sequences = True, stateful = True))\n",
    "\n",
    "    lstm_network.append(tf.keras.layers.Dense(flatten_dims))\n",
    "    lstm_network.append(tf.keras.layers.LeakyReLU())\n",
    "\n",
    "    output_encoder = []\n",
    "    \"\"\"building an output decoder, which is made of deconvolutional layers\"\"\"\n",
    "    for config in conv_transpose_params:\n",
    "      (filters, kernel_size, strides) = config\n",
    "      output_encoder.append(\n",
    "            tf.keras.layers.Conv2DTranspose(\n",
    "                filters=filters,\n",
    "                kernel_size=kernel_size,\n",
    "                strides=strides,\n",
    "                dilation_rate=(1,1),\n",
    "                padding = 'same'))\n",
    "      output_encoder.append(tf.keras.layers.BatchNormalization(momentum=0.99))\n",
    "      output_encoder.append(tf.keras.layers.LeakyReLU())\n",
    "      #output_encoder.append(tf.keras.layers.Dropout(0.2))\n",
    "    \n",
    "    output_encoder.append(tf.keras.layers.Conv2DTranspose(filters =3, kernel_size = (1,1), strides = (1,1), dilation_rate=(1,1),padding = 'same', activation='sigmoid'))\n",
    "    \n",
    "    self.input_encoder = input_encoder\n",
    "    self.lstm_network = lstm_network\n",
    "    self.output_encoder = output_encoder\n",
    "    \n",
    "\n",
    "  def call(self, state):\n",
    "      \"\"\"reshaping data to make it appropriate for an LSTM network\"\"\"\n",
    "      if tf.rank(state) == 3:\n",
    "        state = tf.reshape(state, (1,1,self.n_actions[0],self.n_actions[1],self.n_actions[2]))\n",
    "      else:\n",
    "        state = tf.reshape(state, (1,len(state),self.n_actions[0],self.n_actions[1],self.n_actions[2]))\n",
    "      action_value = self.input_encoder[0](state)\n",
    "      input_shape = np.shape(action_value)\n",
    "      \"\"\"putting data through the input encoder\"\"\"\n",
    "      for i in range(1,len(self.input_encoder)):\n",
    "        action_value = self.input_encoder[i](action_value)\n",
    "      \"\"\"reshaping data to be suitable for lstm network\"\"\"\n",
    "      action_value = tf.reshape(action_value, (1,input_shape[1],self.flatten_dims))\n",
    "      for layer in self.lstm_network:\n",
    "        action_value = layer(action_value)\n",
    "      \"\"\"reshaping data to be suitable for the deconvolutional layers\"\"\"\n",
    "      action_value = tf.reshape(action_value, (input_shape[1],self.reshape_dims[0],self.reshape_dims[1],256))\n",
    "      for layer in self.output_encoder:\n",
    "        action_value = layer(action_value)\n",
    "\n",
    "      return action_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2640d424",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ppo_agent:\n",
    "  def __init__(self, n_actions, conv_layer_params, conv_transpose_params, gamma = 0.5, alpha = 0.0003, policy_clip = 0.2, batch_size = 8,\n",
    "               n_epochs = 5, gae_lambda = 0.95, N = 10):\n",
    "    \n",
    "    self.gamma = gamma\n",
    "    self.n_actions = n_actions\n",
    "    self.alpha = alpha\n",
    "    self.policy_clip = policy_clip\n",
    "    self.batch_size = batch_size\n",
    "    self.N = N\n",
    "    self.n_epochs = n_epochs\n",
    "    self.gae_lambda = gae_lambda\n",
    "\n",
    "    self.actor = actor_network(n_actions=(64,64,3), conv_layer_params=conv_layer_params, conv_transpose_params=conv_transpose_params, name=os.path.join('experimental_condition_four','actor_network'))\n",
    "    self.critic = critic_network(n_actions=(64,64,3), conv_layer_params=conv_layer_params, conv_transpose_params=conv_transpose_params, name=os.path.join('experimental_condition_four','critic_network'))\n",
    "    \n",
    "    self.actor.compile(optimizer = tf.keras.optimizers.Adam(learning_rate=alpha))\n",
    "\n",
    "    self.critic.compile(optimizer = tf.keras.optimizers.Adam(learning_rate=alpha))\n",
    "\n",
    "    self.memory = ppo_memory(self.batch_size)\n",
    "\n",
    "  def remember(self, state, action, probs, vals, reward, done):\n",
    "    self.memory.store_memory(state, action, probs, vals, reward, done)\n",
    "\n",
    "  def save_models(self):\n",
    "    print('saving models')\n",
    "    self.actor.save_weights(self.actor.checkpoint_file)\n",
    "    self.critic.save_weights(self.critic.checkpoint_file)\n",
    "    vae.autoencoder.save_weights(os.path.join('experimental_condition_four','updated_appraiser.h5'))\n",
    "    \n",
    "\n",
    "\n",
    "  def load_models(self):\n",
    "    print('loading models')\n",
    "    self.actor.load_weights(self.actor.checkpoint_file)\n",
    "    self.critic.load_weights(self.critic.checkpoint_file)\n",
    "    vae.autoencoder.load_weights(os.path.join('experimental_condition_four','updated_appraiser.h5'))\n",
    "\n",
    "\n",
    "  def choose_action(self, observation):\n",
    "    actions = self.actor(observation)\n",
    "    img = (actions.sample())\n",
    "    action = img\n",
    "\n",
    "    value = self.critic(observation)\n",
    "\n",
    "    probs = actions.log_prob(img).numpy()\n",
    "\n",
    "    return (action.numpy()).astype(np.float32),\\\n",
    "    probs,\\\n",
    "    value.numpy()\n",
    "\n",
    "  \n",
    "  def learn(self):\n",
    "    for _ in range(n_epochs):\n",
    "      \n",
    "      state_array, action_array, old_probs_array,\\\n",
    "       vals_array, reward_array, done_array, batches =  self.memory.generate_batches()\n",
    "      \n",
    "      \n",
    "      values = tf.squeeze(vals_array).numpy()\n",
    "\n",
    "\n",
    "      advantage = np.zeros(shape=np.shape(reward_array), dtype=np.float32)\n",
    "\n",
    "      \"\"\"calculating advantage for future time steps from each point in batch\"\"\"\n",
    "      for x in range(len(reward_array)-1):\n",
    "        discount = 1\n",
    "        advantage_t = 0\n",
    "        for y in range(x, len(reward_array)-1):\n",
    "           advantage_t += discount*(reward_array[y] + self.gamma*values[y+1]*\\\n",
    "                            (1-int(done_array[y])) - values[y])\n",
    "           discount *= self.gamma*self.gae_lambda\n",
    "        advantage[x] = advantage_t\n",
    "\n",
    "      \"\"\"training per batch\"\"\"\n",
    "      for batch in batches:\n",
    "        with tf.GradientTape() as t1, tf.GradientTape() as t2:\n",
    "          batch = np.array(batch)\n",
    "          states = tf.convert_to_tensor(state_array[batch])\n",
    "          old_probs = tf.convert_to_tensor(old_probs_array[batch])\n",
    "\n",
    "          actions = tf.convert_to_tensor(action_array[batch])\n",
    "          \"\"\"finding the distributions and values per batch of states\"\"\"\n",
    "          distribution = self.actor(states)\n",
    "\n",
    "          critic_value = tf.squeeze(self.critic(states))\n",
    "          \n",
    "          \"\"\"finding new probability and comparing it to old\"\"\"\n",
    "          new_probs = distribution.log_prob(actions)\n",
    "\n",
    "          probability_ratio = tf.exp(new_probs) / tf.exp(old_probs)\n",
    "\n",
    "          weighted_probs = advantage[batch] * probability_ratio\n",
    "          \"\"\"clipping values\"\"\"\n",
    "          weighted_clipped_probs = tf.clip_by_value(weighted_probs, 1-self.policy_clip, 1+self.policy_clip)*advantage[batch]\n",
    "\n",
    "          returns = advantage[batch]+values[batch]\n",
    "        \n",
    "          \"\"\"calculating losses\"\"\"\n",
    "          critic_loss = ((returns - critic_value)**2)\n",
    "          critic_loss = tf.math.reduce_mean(critic_loss)\n",
    "\n",
    "          actor_loss = tf.math.reduce_mean(-tf.math.minimum(weighted_probs, weighted_clipped_probs))\n",
    "          total_loss = actor_loss + 0.5*critic_loss\n",
    "          \n",
    "\n",
    "          \"\"\"training actor\"\"\"\n",
    "          actor_gradient = t2.gradient(total_loss, self.actor.trainable_variables)\n",
    "          self.actor.optimizer.apply_gradients(zip(actor_gradient, self.actor.trainable_variables))\n",
    "\n",
    "\n",
    "          \"\"\"training critic\"\"\"\n",
    "          critic_gradient = t1.gradient(total_loss, self.critic.trainable_variables)\n",
    "          self.critic.optimizer.apply_gradients(zip(critic_gradient, self.critic.trainable_variables))\n",
    "\n",
    "\n",
    "    self.memory.clear_memory()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "321935c8",
   "metadata": {},
   "source": [
    "Below cell is the equivalent of a config file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6e016a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"change this to reload models later\"\"\"\n",
    "load_models = True\n",
    "\"\"\"frequency with which to display outputs\"\"\"\n",
    "disp_frequency = 25\n",
    "\"\"\"whether to display progress in terms of score\"\"\"\n",
    "verbose = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b887c5cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading models\n"
     ]
    }
   ],
   "source": [
    "\"\"\"creating environment\"\"\"\n",
    "env = gym.make('painting_environment-v1')\n",
    "processed_imgs_two = np.zeros((1,400,800,3), dtype = int)\n",
    "\"\"\"number of actions taken between learning\"\"\"\n",
    "N = 10\n",
    "\"\"\"number of epochs per learning iteration\"\"\"\n",
    "n_epochs = 8\n",
    "\"\"\"learning rate for actor and critic\"\"\"\n",
    "alpha = 0.0001\n",
    "\"\"\"batch size for actor/critic training\"\"\"\n",
    "batch_size = 8\n",
    "learn_iters = 0\n",
    "avg_score = 0\n",
    "n_steps = 0\n",
    "a = 0\n",
    "agent = ppo_agent(n_actions=env.action_space.shape, \n",
    "                      conv_layer_params =[(64, 2, 2), (128, 2, 2),(128,1,1),(128,1,1),(256,1,1)], \n",
    "                      conv_transpose_params=[(256,1,1),(128,1,1),(128,1,1),(128,2,2),(64,2,2)],\n",
    "                      batch_size = batch_size, n_epochs = n_epochs, alpha = alpha)\n",
    "if load_models:\n",
    "        agent.actor(np.zeros((1,64,64,3)))\n",
    "        agent.critic(np.zeros((1,64,64,3)))\n",
    "        agent.load_models()\n",
    "\"\"\"how many episodes\"\"\"\n",
    "n_episodes = 1000\n",
    "best_score = env.reward_range[0]\n",
    "steps_since_improvement = 0\n",
    "score_history = []\n",
    "avg_score_history = []#list(np.load(os.path.join('condition_three','score_history_weights_invis_reviewed.npy')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a75d5767",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning\n",
      "learning\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-fee5b7b98792>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m   \u001b[1;32mwhile\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0maction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprobs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchoose_action\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m64\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m64\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m     \u001b[0mobservation_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m     \u001b[0mn_steps\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0mscore\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreduce_mean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreward\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-6-9b18580fcd07>\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     24\u001b[0m     \u001b[0mvae\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreconstructed_orig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m400\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m400\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m     \u001b[0mnew_weights\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvae\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[0mweight_diff\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mget_weights\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1946\u001b[0m     \"\"\"\n\u001b[0;32m   1947\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdistribute_strategy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1948\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mModel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1949\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1950\u001b[0m   def save(self,\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36mget_weights\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1919\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1920\u001b[0m         \u001b[0moutput_weights\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1921\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mbackend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_get_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_weights\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1922\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1923\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mdoc_controls\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdo_not_generate_docs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\util\\dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    199\u001b[0m     \u001b[1;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    200\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 201\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    202\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    203\u001b[0m       \u001b[1;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\keras\\backend.py\u001b[0m in \u001b[0;36mbatch_get_value\u001b[1;34m(tensors)\u001b[0m\n\u001b[0;32m   3644\u001b[0m   \"\"\"\n\u001b[0;32m   3645\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3646\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtensors\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3647\u001b[0m   \u001b[1;32melif\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minside_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3648\u001b[0m     \u001b[1;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Cannot get value inside Tensorflow graph function.'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\keras\\backend.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m   3644\u001b[0m   \"\"\"\n\u001b[0;32m   3645\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3646\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtensors\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3647\u001b[0m   \u001b[1;32melif\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minside_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3648\u001b[0m     \u001b[1;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Cannot get value inside Tensorflow graph function.'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py\u001b[0m in \u001b[0;36mnumpy\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    616\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    617\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 618\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    619\u001b[0m     raise NotImplementedError(\n\u001b[0;32m    620\u001b[0m         \"numpy() is only available when eager execution is enabled.\")\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mnumpy\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1069\u001b[0m     \"\"\"\n\u001b[0;32m   1070\u001b[0m     \u001b[1;31m# TODO(slebedev): Consider avoiding a copy for non-CPU or remote tensors.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1071\u001b[1;33m     \u001b[0mmaybe_arr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1072\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmaybe_arr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1073\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m_numpy\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1035\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1036\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1037\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_numpy_internal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1038\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1039\u001b[0m       \u001b[0msix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i in range(n_episodes):\n",
    "  observation = env.reset()\n",
    "  done = False\n",
    "  score = 0\n",
    "  while not done:\n",
    "    action, probs, value = agent.choose_action(observation.reshape(1,64,64,3))\n",
    "    observation_, reward, done, info = env.step(action)\n",
    "    n_steps += 1\n",
    "    score = (tf.reduce_mean(reward)).numpy()\n",
    "    score_history.append(score)\n",
    "    agent.remember(observation,\\\n",
    "                   action, probs, value, reward, done)\n",
    "    \n",
    "    \n",
    "    if n_steps % N == 0:\n",
    "      print('learning')\n",
    "      agent.learn()\n",
    "      learn_iters += 1\n",
    "    \n",
    "    if n_steps % disp_frequency == 0:\n",
    "      print('rendering')\n",
    "      plt.figure()\n",
    "      plt.imshow(env.render())\n",
    "      plt.pause(1)\n",
    "      plt.figure()\n",
    "      plt.imshow(action.squeeze())\n",
    "      plt.pause(1)\n",
    "      print(score)\n",
    "        \n",
    "    observation = observation_\n",
    "  \n",
    "  agent.save_models()\n",
    "  avg_score = np.mean(score_history)\n",
    "  avg_score_history.append(avg_score)\n",
    "  score_history = []\n",
    "  if verbose:\n",
    "      print('episode ',str(i), 'score %1f' %score, 'avg_score %1f' %avg_score, 'time steps %1f' %n_steps, '\\n learning iters %1f' %learn_iters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "374022cd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
